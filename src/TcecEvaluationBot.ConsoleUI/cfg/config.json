{
    /*
        Options for disk storage management api.
    */
    "ext" : {
        /*
            Thread pools are used for performing async io.
            There is always one default thread pool
            that is used when nothing else matches.
            Not all drives have to be enumerated,
            in particular it's wasteful to enumerage
            paths that are not used at all by the program.
            A path can be any path, for example
            "C:/tmp" would match all files that are
            children of the tmp directory.
        */
        "thread_pools" : [
            { "threads" : 8, "paths" : ["C:"] },
            { "threads" : 8, "paths" : ["D:", "F:"] },
            { "threads" : 8, "paths" : ["W:"] }
        ],

        /*
            Options for the always present thread pool
            that serves paths that don't have specialized ones.
        */
        "default_thread_pool" : {
            "threads" : 4
        },

        /*
            Sum of the following should be below the limit
            of the standard C file library.
            Pooled files are usually used when the number
            of files to manage is unbounded (so high), hence
            the limit is higher than for unpooled files.
        */
        "max_concurrent_open_pooled_files" : 256,
        "max_concurrent_open_unpooled_files" : 128,

        "merge" : {
            /*
                Maximum number of files being merged at once.
                Should be kept below max_concurrent_open_pooled_files.
                High values may be bad for low seek time drives.
            */
            "max_batch_size" : 128,

            /*
                Specifies the size of each output buffer during
                merge process. There is at most 2 output buffers
                at any given moments.
            */
            "output_buffer_size" : "8MiB",

            /*
                Specifies the size of each input buffer during
                merge process. There is at most max_batch_size
                input buffers at any given moment.
            */
            "input_buffer_size" : "4MiB"
        },

        "equal_range" : {
            /*
                When performing the search we do a lot of random
                file seeks. It's beneficial to read as much as possible
                at each access, but not too much. The best value depends on
                drive characteristics. Going below 32kB is ill advised.
            */
            "max_random_read_size" : "32KiB"
        },

        "index" : {
            /*
                Specifies the size of the read buffer used
                when building an index of a file.
                Index building is fast so this should be
                high enough to maximize disk read speed.
            */
            "builder_buffer_size" : "8MiB"
        }
    },

    /*
        Options for various position database components
    */
    "persistence" : {
        /*
            Specifies the number of bytes that can be used
            by a single header for buffering output when
            a database is being populated.
        */
        "header_writer_memory" : "16MiB",

        /*
            Options for the 'alpha' storage format.
            It uses 20 bytes for each position.
            Database is split among 9 directories - each one
            is managed separately.
        */
        "db_alpha" : {
            /*
                Specifies the density of entries in an index.
                Higher values make the index more sparse.
                Lower values make the index denser.
                The denser the index the more space it takes.
                Should not be higher than max_sequential_read_size/32
                for maximal query performance.
            */
            "index_granularity" : 1024,

            /*
                The total size of the buffers used for file
                merging, specified in bytes.
                Large sizes incur overhead because the whole
                buffer needs to be filled before the merging
                process starts. Optimal values depend on merge
                settings and the total file size. 1GiB is
                good assuming large files (>4GiB in total) are
                being merged.
            */
            "max_merge_buffer_size" : "1GiB",

            /*
                Specifies the number of bytes that can be used
                by a single PGN parser as a buffer.
                This also creates a limit on how much space a single
                PGN game record may take (they are usually at most a few kB).
                Only half of the buffer is used at a time.
                Buffers in the order of a few MiB should be preferred as
                PGN parsing is fast and may run in parallel with other
                disk io.
            */
            "pgn_parser_memory" : "4MiB"
        },

        /*
            Options for the 'beta' storage format.
            It uses 24 bytes for each distinct position.
            It ends up using less space than alpha format
            at around 1 billion total positions - but
            that depends on the number of duplicates.
            Whole database is stored in one partition.
            The amound of disk seeks required for querying is
            around 3 times smaller than for the alpha format.
        */
        "db_beta" : {
            /*
                In this case we always read at least index_granularity entries
                for a single query. 1024 is a good tradeoff between speed and space.
            */
            "index_granularity" : 1024,

            "merge_writer_buffer_size" : "4MiB",

            "pgn_parser_memory" : "4MiB",

            "bcgn_parser_memory" : "4MiB",

            "index_writer_buffer_size" : "4MiB",

            "header_buffer_memory" : "4MiB"
        },

        "db_delta" : {
            /*
                In this case we always read at least index_granularity entries
                for a single query. 1024 is a good tradeoff between speed and space.
            */
            "index_granularity" : 1024,

            "merge_writer_buffer_size" : "4MiB",

            "pgn_parser_memory" : "4MiB",

            "bcgn_parser_memory" : "4MiB",

            "index_writer_buffer_size" : "4MiB",

            "header_buffer_memory" : "4MiB"
        },

        "db_delta_smeared" : {
            /*
                In this case we always read at least index_granularity entries
                for a single query. 1024 is a good tradeoff between speed and space.
            */
            "index_granularity" : 1024,

            "merge_writer_buffer_size" : "4MiB",

            "pgn_parser_memory" : "4MiB",

            "bcgn_parser_memory" : "4MiB",

            "index_writer_buffer_size" : "4MiB",

            "header_buffer_memory" : "4MiB"
        },

        "db_epsilon" : {
            /*
                In this case we always read at least index_granularity entries
                for a single query. 1024 is a good tradeoff between speed and space.
            */
            "index_granularity" : 1024,

            "merge_writer_buffer_size" : "4MiB",

            "pgn_parser_memory" : "4MiB",

            "bcgn_parser_memory" : "4MiB",

            "index_writer_buffer_size" : "4MiB",

            "header_buffer_memory" : "4MiB"
        },

        "db_epsilon_smeared_a" : {
            /*
                In this case we always read at least index_granularity entries
                for a single query. 1024 is a good tradeoff between speed and space.
            */
            "index_granularity" : 1024,

            "merge_writer_buffer_size" : "4MiB",

            "pgn_parser_memory" : "4MiB",

            "bcgn_parser_memory" : "4MiB",

            "index_writer_buffer_size" : "4MiB",

            "header_buffer_memory" : "4MiB"
        }
    },

    "command_line_app" : {

        "import_memory" : "2GiB",
        "pgn_parser_memory" : "4MiB",
        "bcgn_parser_memory" : "4MiB",

        /*
            Options for the dump command
        */
        "dump" : {
            "import_memory" : "2GiB",
            "pgn_parser_memory" : "4MiB",
            "bcgn_parser_memory" : "4MiB",
            "max_merge_buffer_size" : "1GiB"
        }
    },

    "console_app" : {
        /*
            Total memory used for buffers when importing PGN
            files into the database.
            Should be kept as high as possible.
            On windows values around 65-75% of free memory are recommended.
            Values higher than that can inhibit system's
            disk read-ahead/file buffering and/or cause
            stalls due to large memory allocations causing
            movement to pagefile.
            Generally, higher values can decrease disk seeks.
            Higher values also result in the importer creating
            less but bigger files. This is benefial both for
            querying and merging performance.
            For the 'alpha' format individual files have size
            about 1/9 of pgn_import_memory.
            For the 'beta' format individual files have size
            about 1/6 of pgn_import_memory.
        */
        "import_memory" : "2GiB",
        "pgn_parser_memory" : "4MiB",
        "bcgn_parser_memory" : "4MiB"
    }
}